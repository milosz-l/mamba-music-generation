defaults:
  - model: mamba
  - _self_

data:
  path: data
  dataset_name: maestro
  tokenizer: remi
  train_split: 0.8

training:
  num_workers: 4 # lower this setting in case of workers segmentation fault error
  batch_size: 3
  epochs: 100
  learning_rate: 5e-4
  step_size: 3
  gamma: 0.1
  callbacks:
    patience: 2

wandb:
  project: "WIMU mamba-music-generation"
  cleanup:
    dry_run: False
    run_cleanup_every_epochs: 5

models:
  save_path: models

inference:
  wandb_model_full_name: "" # example: "wut-zzsn/WIMU mamba-music-generation/model-mn6cowxw:v4"
  model_path: "models/model-mn6cowxw:v4.ckpt" # optional, if you want to load model that is not in wandb fill this and leave wandb_model_full_name empty
  input_length: 1   # Example length
  input_ids: [1]      # list of input ids (optional, if not provided, random input will be generated)
  max_length: 10
  temperature: 0.2
  top_k: 50