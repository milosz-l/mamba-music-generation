defaults:
  - model: mamba
  - _self_

data:
  path: data
  dataset_name: maestro
  tokenizer: remi
  train_split: 0.8

training:
  num_workers: 4 # lower this setting in case of workers segmentation fault error
  batch_size: 3
  epochs: 100
  learning_rate: 5e-4
  step_size: 3
  gamma: 0.1
  callbacks:
    patience: 4

wandb:
  project: "WIMU mamba-music-generation"
  cleanup:
    dry_run: False
    run_cleanup_every_epochs: 5

models:
  save_path: models

inference:
  wandb_run_id: ""
  model_path: "models/lyric-wildflower-36_model.pt" # optional, if you want to load model that is not in wandb fill this and leave wandb_run_id empty
  input_length: 1   # Example length
  input_ids: [1]      # list of input ids (optional, if not provided, random input will be generated)
  max_length: 10
  temperature: 0.2
  top_k: 50