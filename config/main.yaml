defaults:
  - model: mamba
  - _self_

data:
  path: /net/tscratch/people/plgmiloszl/mamba-music-generation/data
  dataset_name: maestro
  tokenizer:
    type: tsd  # remi | midiike | tsd, structured | cpword | octuple | mumidi | mmm
    training_model: BPE  # BPE | Unigram | WordPiece
  train_split: 0.8
  max_seq_len: 1024
  test_train_on_one_file: False  # set True if you want to overtrain on one song
  test_train_on_one_file_path: ""  # empty if you want to use random song

training:
  num_workers: 2 # lower this setting in case of workers segmentation fault error
  batch_size: 4
  epochs: 10
  learning_rate: 1e-4
  step_size: 3
  gamma: 0.1
  callbacks:
    patience: 4


wandb:
  project: "WIMU mamba-music-generation"
  entity: "wut-zzsn"
  cleanup:
    dry_run: False
    run_cleanup_every_epochs: 5

models:
  save_path: models

inference:
  wandb_model_full_name: "wut-zzsn/WIMU mamba-music-generation/model-wnz2pevd:v2" # example: "wut-zzsn/WIMU mamba-music-generation/model-mn6cowxw:v4"
  model_path: "models/model-mn6cowxw:v4.ckpt" # optional, if you want to load model that is not in wandb fill this and leave wandb_model_full_name empty
  input_length: 1   # Example length
  max_length: 1024
  temperature: 0.8
  top_k: 70
  repetition_penalty: 0.9